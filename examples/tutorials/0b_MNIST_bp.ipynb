{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 0b: **backpropagation with pcax**\n",
    "In this notebook, you will learn how to use pcax to a network equivalent to the one in *Tutorial 0*, but trained using backpropagation.\n",
    "The general idea is to simply remove all the bits related to predictive coding, nothing more than that.\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switched to CUDA 11.7.\n"
     ]
    }
   ],
   "source": [
    "!source switch-cuda 11.7\n",
    "\n",
    "# Core dependencies\n",
    "import jax\n",
    "import optax\n",
    "\n",
    "# pcax\n",
    "import pcax as px # same as import pcax.pc as px\n",
    "import pcax.nn as nn\n",
    "from pcax.core import _ # _ is the filter object, more about it later!\n",
    "import pcax.interface as pxi # remember it will be soon deprecated\n",
    "\n",
    "import numpy as np\n",
    "from torchvision.datasets import MNIST\n",
    "import timeit\n",
    "import os\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\" # remember this line if you are sharing the GPU with others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step: removing all pc layers from the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(px.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, nm_layers=2) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.act_fn = jax.nn.tanh\n",
    "\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.linear_h = nn.ModuleList(\n",
    "            [nn.Linear(hidden_dim, hidden_dim) for _ in range(nm_layers)]\n",
    "        )\n",
    "        self.linear2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = self.act_fn(self.linear1(x))\n",
    "\n",
    "        for i in range(len(self.linear_h)):\n",
    "            x = self.act_fn(self.linear_h[i](x))\n",
    "\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second step: we don't need pc related params:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"batch_size\": 256,\n",
    "    # \"x_learning_rate\": 0.01,\n",
    "    \"w_learning_rate\": 1e-3,\n",
    "    \"num_epochs\": 4,\n",
    "    \"hidden_dim\": 128,\n",
    "    \"input_dim\": 28 * 28,\n",
    "    \"output_dim\": 10,\n",
    "    \"seed\": 0,\n",
    "    # \"T\": 4,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No changes in the data loading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(t, k):\n",
    "    return np.array(t[:, None] == np.arange(k), dtype=np.float32)\n",
    "\n",
    "\n",
    "class FlattenAndCast:\n",
    "    def __call__(self, pic):\n",
    "        return np.ravel(np.array(pic, dtype=np.float32) / 255.0)\n",
    "\n",
    "\n",
    "train_dataset = MNIST(\n",
    "    \"/tmp/mnist/\",\n",
    "    transform=FlattenAndCast(),\n",
    "    download=True,\n",
    "    train=True,\n",
    ")\n",
    "train_dataloader = pxi.data.Dataloader(\n",
    "    train_dataset,\n",
    "    batch_size=params[\"batch_size\"],\n",
    "    num_workers=16,\n",
    "    shuffle=True,\n",
    "    persistent_workers=True,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "\n",
    "test_dataset = MNIST(\n",
    "    \"/tmp/mnist/\",\n",
    "    transform=FlattenAndCast(),\n",
    "    download=True,\n",
    "    train=False,\n",
    ")\n",
    "test_dataloader = pxi.data.Dataloader(\n",
    "    test_dataset,\n",
    "    batch_size=params[\"batch_size\"],\n",
    "    num_workers=4,\n",
    "    shuffle=False,\n",
    "    persistent_workers=False,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is still a `px.Module` so it needs to be in the global scope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(28 * 28, params[\"hidden_dim\"], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the predict function we don't need the target (it was used to fix the last pc layer which is not there anymore)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@px.vectorize(_(px.NodeVar), in_axis=(0,)) # note the change to `in_axis`\n",
    "@px.bind(model)\n",
    "def predict(x):\n",
    "    return model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss is slightly different, as now we use the target to compute the actual loss (as we normally do in backpropagation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@px.vectorize(_(px.NodeVar), in_axis=(0, 0), out_axis=(\"sum\", 0))\n",
    "@px.bind(model)\n",
    "def loss(x, t):\n",
    "    y = model(x)\n",
    "    # we also return y even if we will not use it, to show how it can be done. Note the change in out_axis.\n",
    "    return ((y - t) ** 2).sum(), y # it's MSE loss without the Mean, since in pc we use the sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only need one gradient function and a single optimizer (since we only optimize the weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = px.gradvalues(\n",
    "    _(px.TrainVar)\n",
    ")(loss)\n",
    "\n",
    "\"\"\"\n",
    "There's actually no need to call predict here as nothing needs to be initialized,\n",
    "however it is good practice to do so as it may be necessary in the general case.\n",
    "\"\"\"\n",
    "with px.eval(model):\n",
    "    predict(np.zeros((params[\"batch_size\"], 28 * 28)))\n",
    "    optim = px.Optim(\n",
    "        optax.adam(params[\"w_learning_rate\"]),\n",
    "        model.vars(_(px.TrainVar)),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training and evaluation functions are very intuitive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@px.jit()\n",
    "@px.bind(model, optim)\n",
    "def train_on_batch(x, y):\n",
    "    \"\"\"\n",
    "    Again, we don't need to call predict since there's no value nodes to be initialized.\n",
    "    If we need the predicted y_hat we can use the value returned by the 'train' function.\n",
    "    \"\"\"\n",
    "    with px.train(model):\n",
    "        g, (v, y_hat) = train(x, y)\n",
    "        optim(g)\n",
    "\n",
    "\n",
    "@px.jit()\n",
    "@px.bind(model)\n",
    "def evaluate(x, y):\n",
    "    with px.eval(model):\n",
    "        y_hat, = predict(x) # remeber that 'predict' always returns a tuple\n",
    "\n",
    "    return (y_hat.argmax(-1) == y.argmax(-1)).mean()\n",
    "\n",
    "\n",
    "def epoch(dl):\n",
    "    for batch in dl:\n",
    "        x, y = batch\n",
    "        y = one_hot(y, 10)\n",
    "\n",
    "        train_on_batch(x, y)\n",
    "\n",
    "    return 0\n",
    "\n",
    "\n",
    "def test(dl):\n",
    "    accuracies = []\n",
    "    for batch in dl:\n",
    "        x, y = batch\n",
    "        y = one_hot(y, 10)\n",
    "\n",
    "        accuracies.append(evaluate(x, y))\n",
    "\n",
    "    return np.mean(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switched to CUDA 11.7.\n",
      "Compiling + Epoch 1 took 4.027969808958005 seconds\n",
      "An Epoch takes on average 0.691992550244322 seconds\n",
      "Final Accuracy: 0.9625401\n"
     ]
    }
   ],
   "source": [
    "!source switch-cuda 11.7\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    t = timeit.timeit(lambda: epoch(train_dataloader), number=1)\n",
    "    print(\"Compiling + Epoch 1 took\", t, \"seconds\")\n",
    "\n",
    "    # Time of an epoch (without jitting)\n",
    "    t = timeit.timeit(lambda: epoch(train_dataloader), number=params[\"num_epochs\"]) / params[\"num_epochs\"]\n",
    "    print(\"An Epoch takes on average\", t, \"seconds\")\n",
    "\n",
    "    print(\"Final Accuracy:\", test(test_dataloader))\n",
    "\n",
    "    del train_dataloader\n",
    "    del test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything is all right, you should get a final accuracy of ~96% and a training time of ~0.7 second per epoch. The training time is actually heavily bottlenecked by the data transfer between CPU and GPU, that's why we are using 16 workers in the dataloader. So depending on your configuration the final speed my change.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('pcax')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f4fd8fe68194e475a108d2c5b5ee1f820870a7a5127298df9ccb3dbbd47c3153"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
